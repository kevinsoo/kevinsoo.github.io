<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="//gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.54.0" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>How wrong were the polls? &middot; Casual Inference</title>

  
  <link type="text/css" rel="stylesheet" href="/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="/css/poole.css">
  <link type="text/css" rel="stylesheet" href="/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">

  
  <link rel="icons8-scatter-plot-90" sizes="144x144" href="/icons8-scatter-plot-90.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="" rel="alternate" type="application/rss+xml" title="Casual Inference" />

  
</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="/"><h1>Casual Inference</h1></a>
      <p class="lead">
      Adventures of a Data Scientist <a href="http://twitter.com/civisanalytics">@CivisAnalytics</a>. Occasional causal inferences.
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        
        <li><a href="/about/"> About </a></li><li><a href="/post/"> Posts </a></li><li><a href="/categories/"> Categories </a></li><li><a href="/tags/"> Tags </a></li>
      </ul>
    </nav>
    
    <div>
        <h3 class="white">Links</h3>
        <i class="fab fa-twitter fa-fw"></i> <a href="http://twitter.com/kevinwxsoo">Twitter</a><br>
        <i class="fab fa-github fa-fw"></i> <a href="https://github.com/kevinsoo">GitHub</a><br>
        <i class="far fa-envelope fa-fw"></i> <a href="mailto:kevinsoo87@gmail.com?Subject=Hello">Email</a><br>
        <br>
    </div>

    <p>&copy; 2019. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="post">
  <h1>How wrong were the polls?</h1>
  <time datetime=2016-12-11T00:00:00Z class="post-date">Sun, Dec 11, 2016</time>
  


<p>Going into the 2016 Presidential Election, most pollsters were <a href="http://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html">confident</a> of a Clinton win. The aftermath of Trump’s win resulted in many questions being asked of pollsters and speculation as to how so many got it wrong. I won’t get into the reasons why; <a href="http://fivethirtyeight.com/features/the-polls-missed-trump-we-asked-pollsters-why/">here</a> are <a href="http://www.vox.com/mischiefs-of-faction/2016/11/9/13574824/polls-clinton-trump">some</a> <a href="http://www.theatlantic.com/politics/archive/2016/11/what-went-wrong-polling-clinton-trump/507188/">articles</a> with <a href="https://www.theguardian.com/commentisfree/2016/nov/09/polls-wrong-donald-trump-election">coverage</a> <a href="http://www.nytimes.com/interactive/2016/11/13/upshot/putting-the-polling-miss-of-2016-in-perspective.html">on</a> <a href="http://www.npr.org/2016/11/14/502014643/4-possible-reasons-the-polls-got-it-so-wrong-this-year">that</a>. Instead, I want to focus on quantifying and visualizing the amount of error in the polls – where were they wrong, and how were they wrong? I used polling data collected by <a href="http://projects.fivethirtyeight.com/general-model/president_general_polls_2016.csv">FiveThirtyEight</a> and scraped the final results from <a href="https://docs.google.com/spreadsheets/d/133Eb4qQmOxNvtesw2hdVns073R68EZx4SfCnP4IGQf8/htmlview?sle=true">David Wasserman</a>.</p>
<p><em>Note: FiveThirtyEight’s dataset of poll results includes both raw and adjusted results. <a href="http://fivethirtyeight.com/features/a-users-guide-to-fivethirtyeights-2016-general-election-forecast/">Adjusted results</a> correct for historical biases in different pollsters, and can be useful for eliminating some noise from our visualizations (assuming we trust FiveThirtyEight’s adjustments). However, because I’m primarily interested in visualizing how the polls actually did, my analyses make use of the raw data, unless stated otherwise.</em></p>
<div id="the-popular-vote-margin" class="section level2">
<h2>The popular-vote margin</h2>
<p>How well did the polls predict the margin between Trump and Clinton in the popular vote? The following plot shows the distribution of projected margins from national-level polls conducted on likely voters in the two weeks prior to Election Day (<em>n</em> = 348). In the final popular-vote tally, Clinton beat Trump by a margin of 2.1% (more than 2.86 million votes). The projected margin (estimated from the average of the polls in the final two weeks) was 2.2%. The polls appeared to do pretty well here – the projected margin was not significantly different from the final (actual) margin.</p>
<p><img src="/post/2016-12-11-how-wrong-were-the-polls_files/figure-html/difference-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="error-in-popular-vote-support-for-each-candidate" class="section level2">
<h2>Error in popular-vote support for each candidate</h2>
<p>The national-level polls in that final period did pretty well in estimating the final popular vote <em>margin</em>, but what about the absolute level of support for each candidate? A 2% margin can mean Clinton is polling at 48% vs. Trump’s 46%, or 40% vs. 38%. The estimated margin can be pretty accurate even if the projected vote-shares for the candidates are off, provided they are both off in the same direction (i.e. the polls were not systematically biased for one candidate but not another).</p>
<p>The polls underestimated the vote-shares of both Clinton and Trump. If the polls were not biased, the error for both candidates in all polls would be centered around zero. In the following plot, we see that the errors were consistently above zero. Error was the difference between the final and predicted vote-share (from a given poll) for a particular candidate – positive (negative) error means a poll underestimated (overestimated) the final outcome.</p>
<p><img src="/post/2016-12-11-how-wrong-were-the-polls_files/figure-html/errors-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Visually, it appears that slightly more polls overestimated Trump than Clinton (Trump’s distribution shows slightly more polls within the range of negative errors). Most polls underestimated both candidates. However, it seems that the largest number of polls underestimating Clinton did so by around 4 percentage points (notice the peak of Clinton’s density plot), while more polls that underestimated Trump did so by larger amounts (notice the fatter tail of Trump’s plot, indicating positive skew in the errors).</p>
<p>From this sample of polls in the two weeks immediately preceeding Election day, national-level polls were generally accurate about the margin between the candidates (projecting ~2% lead for Clinton), but underestimated the vote-shares of both candidates. Polls underestimated Clinton mostly by about 4 percentage points, while there was more variability in how much they underestimated Trump. Quite a few polls underestimated him by quite a bit – by more than 5 percentage points!</p>
</div>
<div id="popular-vote-share-over-the-long-campaign" class="section level2">
<h2>Popular-vote share over the (long) campaign</h2>
<p>The campaign-season is <a href="https://www.washingtonpost.com/news/the-fix/wp/2016/05/04/why-do-american-elections-last-so-long/?utm_term=.5d1647b25296">extremely</a> <a href="http://www.theatlantic.com/international/archive/2016/10/us-election-longest-world/501680/">long</a>. The first poll in our dataset was finished on November 8, 2015 – a full year before Election Day. Polls taken early on should be <a href="http://fivethirtyeight.com/features/a-year-out-ignore-general-election-polls/">less predictive</a> of the final outcome. The reasons are complex, but at the most general level:</p>
<ul>
<li>Fewer voters have made up their minds when early polls are conducted</li>
<li>Polling at an early date leaves more time for respondents to change their minds before Election Day</li>
<li>There is time for events to affect the (still) fluctuating support-levels of candidates</li>
</ul>
<p>Looking at all the polls in our dataset (from a year out), we would expect to see that as Election Day approaches, projected vote-shares for each candidate will approximate the final vote-share. The following graph plots the projected vote-shares by candidate over time. Because this time-series graph contains so many data points and so much noise, I’ve used the <em>adjusted</em> polling results (the picture doesn’t change significantly with the raw data, but some polls have more extreme results).</p>
<p><img src="/post/2016-12-11-how-wrong-were-the-polls_files/figure-html/errorTime-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Two things seem pretty clear. First, the number of polls increased from around July 2016 – more data theoretically means less noise. Second, a rolling average of candidate’s projected vote-shares in each poll showed more variation further back in time before settling into the ~2% margin from September 2016 (though these projections underestimated the final vote-share, as described above).</p>
<p>In future analyses, I intend to investigate changes at particular points in time (perhaps in light of specific events, like each of the presidential debates, or the breaking of certain news stories pertaining to each candidate).</p>
</div>
<div id="errors-in-state-level-polls" class="section level2">
<h2>Errors in state-level polls</h2>
<p>While national-level polling doesn’t seem to have done too badly in predicting the popular-vote margin, errors in state-level polls can be more costly because of how the Electoral College works. Ultimately, whether a candidate wins the popular vote <em>within</em> a state determines whether they receive the electoral votes they need (with the exception of a couple of states). How well did state-level polls do in estimating the margins in the state-level popular vote? Were polls in some states especially biased?</p>
<p>With any poll, there will be some over or underestimation of the margin. If the actual margin in a given state is a Trump win by ~ 2%, a poll biased towards Trump (projecting a win of &gt; 2%) may have a projected margin that’s too high, but it would still project the right outcome (Trump wins). A poll biased towards Clinton could project the right outcome if the bias is small enough (if the bias is 1% towards Clinton, then Trump shoulds win by ~ 1%), or it could get the outcome wrong if the bias is large enough (if the bias is 3% towards Clinton, then Clinton should win by ~ 1%).</p>
<p>If the polls were all equally accurate, we would expect that the distribution of bias to be symmetrical around 0 (though no single poll would have exactly 0 bias, the average bias of all the polls across all the states should come close). We woud also expect that across states won by each candidate, the projected margin will sometimes be overestimated, and sometimes be underestimated.</p>
<p>The following plot displays the errors between the actual state-level popular vote margin and the margin projected from polls in the last two weeks of the campaign. The separate plots are for states won by either candidate, and they’re arranged from states where Trump was most underestimated to states where Clinton was most underestimated.</p>
<p><img src="/post/2016-12-11-how-wrong-were-the-polls_files/figure-html/differenceState-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>A few things jump out from this plot:</p>
<ul>
<li>The majority of polls underestimated Trump (ie. had positive errors), except for 11 states that either got the margin right (Colorado and Washington, both correctly projecting a Clinton win) or underestimated Clinton (i.e. had positive errors) – New York, Massachusetts, Connecticut, Nevada, Illinois, New Mexico, Rhode Island, California, Hawaii, and the District of Columbia.</li>
<li>Looking only at states that Clinton won, polling error was distributed quite symmetrically – she was overestimated in some states, underestimated in others. This is what you’d expect for both candidates and the set of states they won if there was nothing systematically wrong with the polls.</li>
<li>Trump was underestimated in every single state that he won, suggesting there was a systematic bias in polling within those states. If there was no systematic bias, we would expect the errors to be distributed more evenly around 0.</li>
</ul>
<p>In states where polls showed a Trump victory, the actual margin would be larger than projected. In states where polls showed a narrow Clinton victory, the amount of bias played a big role – if the polling was biased against Trump sufficiently, the actual outcome would be flipped. This mattered most of all in several close swing states where Clinton was expected to win but ended up losing.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>As stated above, there’s been a ton of analysis about how the polls missed Trump’s win. This post has avoided those issues, except to show that there are specific ways that the polls did and did not get things right. I’m planning to write more on the data presented here – I would love to talk if you have any ideas for interesting analyses!</p>
</div>

</div>


    </main>

    
  </body>
</html>
